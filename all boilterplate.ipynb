{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General outline:\n",
    "\n",
    "**~15-20 min: Data preparation**\n",
    "- set prediction goals, decide on a metric to evaluate the model on (eg. AUC-ROC, adjusted R^2)\n",
    "- load the data, note the size and composition (continuous vs categorical features)\n",
    "- broadly segregate continuous vs categorical variables\n",
    "- get quick summary stats for continuous variables and value counts for categorical variables\n",
    "- treatment of invalid/missing data (including in the target)\n",
    "  - drop columns like ids, dates(if underlying info like day/year/month also irrelevant), and const values\n",
    "  - remove extreme values in case of regression targets (use a boxplot)\n",
    "  - use imputers to fill in missing values (eg. Simple/KKN-Imputer - may need to encode categorical features first)\n",
    "- visualize the data to get a feel of it\n",
    "  1. visualize the target (histogram/bar for regression/classification) - note if you may need to apply any transformations (like log-transform for regression target)\n",
    "  2. visualize the univariate distributions of the features (histograms, bars)\n",
    "  3. visulize target vs feature values distributions (maybe skip for the first pass)\n",
    "      - for **cassification**\n",
    "        - *categorical variables* - by doing nested bar plots with outer class being the target \n",
    "        - *continuous variables* - by plotting histograms with distributions colored by class `df['col'].hist(by=df['target'])`\n",
    "      -  for **regression**\n",
    "        - *categorical variables* - box plots by category for different categorical variables\n",
    "        - *continous variables* - good ol' scatter plots\n",
    "\n",
    "**~5-10min: Establish baseline** \n",
    "- treatment of categorical variables. For example, OrdinalEncoder, OneHotEncoder, categorical_encoders.TargetEncoder (needs to go separately and first in the pipeline)\n",
    "- treatment of continous variables if needed for the model:\n",
    "  - definitely scale for KNN, Kernel SVM: `StandardScaler` or `MinMaxScaler` ftw\n",
    "- split into train/test data (80/20), and fit a couple simple models (eg. linear - ridge/lasso, SVM etc)\n",
    "\n",
    "**~5-10min: Improve upon the baseline**\n",
    "- adapt feature engineering  \n",
    "- train RFs, GBMs(LightGBM) to hopefully beat the simple models' scores\n",
    "- run grid-search CV on the hyperparameter space and get the best estimator scores for the two\n",
    "- see the performance over the grid search to sanity check/narrow down the neighborhood\n",
    "```\n",
    "    plt.title(\"Grid Search Results for Ridge Regression\")\n",
    "\n",
    "    ridge_alphas = [param['ridge__alpha'] for param in ridge_grid.cv_results_['params']]\n",
    "    plt.plot(ridge_alphas, ridge_grid.cv_results_['mean_test_score'], marker='o')\n",
    "```\n",
    "\n",
    "**~10 min: Evaluation**\n",
    "- evaluation for the model on the held-out test set\n",
    "- plotting pred vs actual target values for *regression*, or seeing class-wise classification report for *classification*\n",
    "\n",
    "**Nice-to-do if time permits**\n",
    "- visualize feature vs target distributions to get more insight into the data\n",
    "- get feature importances for the model - using either native implementations (coef, gini gains) or SHAP\n",
    "\n",
    "### Hyperparameters for common models:\n",
    "- `sklearn.linear_model.Ridge` - alpha in default CV (0.1, 1.0, 10.0)\n",
    "- `sklearn.linear_model.LogisticRegression` - C value (inverse of regularization strength), tol\n",
    "- `sklearn.svm.SVC` {'svc__C': np.logspace(-3, 2, 6), 'svc__gamma':\n",
    "(gamma parameter in RBF directly relates to scaling of data and n_features – the default is 1/(X.var() * n_features))\n",
    "\n",
    "**Random Forests**\n",
    "  - Main parameter: max_feature\n",
    "  - Around sqrt(n_features) for classification\n",
    "  - Around n_features for regression\n",
    "  - n_estimators > 100\n",
    "  - Other - max_depth, max_leaf_nodes, min_samples_split again\n",
    "\n",
    "**Gradient Boosting**\n",
    "- no. of trees\n",
    "- learning rate\n",
    "- depth of trees\n",
    "\n",
    "**Thumb rule for when to use tree-based models:**\n",
    "- model non-linear relationships\n",
    "- don’t care about scaling, no need for feature engineering\n",
    "- random forests are very robust, good benchmark\n",
    "- ***gradient boosting*** will often give the best performance with careful tuning (*early stopping, learning rate, regularization, max_features*, pruning via *max_depth*)\n",
    "- LightGBM Interface:\n",
    "```\n",
    "    lgbm = LGBMClassifier()\n",
    "    lgbm.fit(X_train, y_train, feature_name=list_of_all_feature_names, categorical_feature=list_of_cat_feature_names)\n",
    "    lgbm.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "##### more miscellaneous TIPs\n",
    "- LinearSVC, LogisticRegression: `dual=False` if `n_samples` >> `n_features`\n",
    "- `LogisticRegression(solver=\"sag\")` for `n_samples` large.\n",
    "- Stochastic Gradient Descent for `n_samples` really large\n",
    "- tip on solvers: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions\n",
    "- LogisticRegression(solver='lbfgs', multi_class='multinomial').fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional TODO**\n",
    "- write helper functions for: visualization (see cases above), maybe using seaborn if seems quicker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "(Will help remind where to look for in the sklearn docs for the right API)\n",
    "\n",
    "**general**\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "**missing values imputation**\n",
    " ```\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    " ```\n",
    "\n",
    "**feature engineering**\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "\n",
    "**cross-validation and model tuning**\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "```\n",
    "\n",
    "**classifiction models**\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "\n",
    "**regression models**\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "```\n",
    "\n",
    "**class imbalance**\n",
    "```\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "```\n",
    "\n",
    "**target transformation** \\\n",
    "`from sklearn.compose import TransformedTargetRegressor`\n",
    "\n",
    "**PCA** \\\n",
    "Example:\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "pca_lr = make_pipeline(StandardScaler(), PCA(n_components=2), LogisticRegression(C=10000))\n",
    "pca_lr.fit(X_train, y_train)\n",
    "print(pca_lr.score(X_train, y_train))\n",
    "print(pca_lr.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "**Sklearn API reference** - https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "loading data \\\n",
    "`df = pd.read_csv()`\n",
    "\n",
    "examining data\n",
    "```\n",
    "df.info()\n",
    "df.head()\n",
    "y.unique()\n",
    "y.nunique()\n",
    "y.value_counts()\n",
    "```\n",
    "\n",
    "train_test_split\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# make y boolean when it's not (eg, -1/1)\n",
    "# this allows sklearn to determine the positive class more easily\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n",
    "```\n",
    "\n",
    "boxplot \\\n",
    "`plt.boxplot(y, vert=False)`\n",
    "\n",
    "apply function \\\n",
    "`df[\"column_name\"].apply(func)`\n",
    "\n",
    "OneHotEncoder \n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories=[['first','second','third','forth']], drop=\"if_binary\")\n",
    "X = [['third'], ['second'], ['first']]\n",
    "enc.fit(X)\n",
    "print(enc.transform([['second'], ['first'], ['third'],['forth']]))\n",
    "```\n",
    "\n",
    "OrdinalEncoder\n",
    "```\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder(categories=[['first','second','third','forth']])\n",
    "X = [['third'], ['second'], ['first']]\n",
    "enc.fit(X)\n",
    "print(enc.transform([['second'], ['first'], ['third'],['forth']]))\n",
    "```\n",
    "\n",
    "ColumnTransformer syntax <-> Pipeline syntax\n",
    "\n",
    "```\n",
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NA'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler())\n",
    "```\n",
    "\n",
    "make_column_selector\n",
    "```\n",
    "preprocess = make_column_transformer(\n",
    "    (cat_preprocessing, make_column_selector(dtype_include='object')),\n",
    "    remainder=cont_preprocessing)\n",
    "```\n",
    "\n",
    "Target transformer (also returns an estimator)\n",
    "```\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    regressor=LinearRegression(),\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp.fit())\n",
    "```\n",
    "\n",
    "Pipeline syntax <-> ColumnTransformer syntax \\\n",
    "`make_pipeline(preprocess, log_regressor)`\n",
    "\n",
    "GridSearchCV syntax \\\n",
    "`GridSearchCV(make_pipeline_output_object, param_grid=param_grid, cv=KFold(n_splits=5,shuffle=True))`\n",
    "\n",
    "Common estimator API:\n",
    "```\n",
    ".fit(X, y)\n",
    ".predict(X')\n",
    ".score(X', y')\n",
    ".predict_proba(X')\n",
    "```\n",
    "\n",
    "CountVectorizer\n",
    "```\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=4)\n",
    "vect.fit(list_of_sentence_strings)\n",
    "print(vect.get_feature_names())\n",
    "X = vect.transform(list_of_sentence_strings)\n",
    "print(X.toarray())\n",
    "```\n",
    "\n",
    "### Evaluation metrics\n",
    "\n",
    "##### Binary classification\n",
    "*Threshold-based:*\n",
    "- accuracy\n",
    "- precision, recall, f1\n",
    "\n",
    "*Ranking:*\n",
    "- average precision\n",
    "- ROC AUC\n",
    "\n",
    "##### Multiclass classification\n",
    "*Threshold-based:*\n",
    "- accuracy\n",
    "- precision, recall, f1 (macro average, weighted)\n",
    "\n",
    "*Ranking:*\n",
    "- OVR ROC AUC\n",
    "- OVO ROC AUC\n",
    "\n",
    "\n",
    "**sklearn**\n",
    "\n",
    "*metrics in cross-validation*\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "explicit_accuracy =  cross_val_score(rf, X, y,  scoring=\"accuracy\")\n",
    "```\n",
    "*common metrics*\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "recall_score(y_test, y_pred)\n",
    "precision_score(y_test, y_pred)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_score = roc_auc_score(y_test, estimator.predict_proba(X_test)[:, 1])\n",
    "classification_report(y_true, y_pred)\n",
    "mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "Plotting the PR curve:\n",
    "```\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    classifier, X_test, y_test, name=\"LinearSVC\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n",
    "```\n",
    "Plotting the ROC curve:\n",
    "```\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "y_score = clf.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "```\n",
    "\n",
    "SpaCy Example usage:\n",
    "```\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"What is my purpose?\")\n",
    "doc.vector\n",
    "```\n",
    "\n",
    "Getting the final set of transformed features and coefficients \n",
    "```\n",
    "def feature_coefficients(grid_search_cv_object):\n",
    "    coeff= grid_search_cv_object.best_estimator_.named_steps['linearsvc'].coef_[0] # key will be the id of the final estimator\n",
    "    feature_names =grid_search_cv_object.best_estimator_.named_steps.columntransformer.named_transformers_[\"onehotencoder\"].get_feature_names()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5 (default, Oct 25 2019, 10:52:18) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5269c0a7ad73348e1a39fbbfe9a8878a0938622bfaba1cb70a06abfc1d5d698f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
