{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General outline:\n",
    "\n",
    "- load the data, note the size and composition (continuous vs categorical features)\n",
    "- set prediction goals, decide on a metric to evaluate the model on (eg. AUC-ROC, adjusted R^2)\n",
    "- broadly segregate continuous vs categorical variables\n",
    "- visualize the data to get a feel of it\n",
    "  1. visualize the target (histogram/bar for regression/classification) - note if you may need to apply any transformations\n",
    "  2. visualize the univariate distributions of the features (histograms, bars)\n",
    "  3. visulize target vs feature values distributions\n",
    "      - for **cassification**\n",
    "        - *categorical variables* - by doing nested bar plots with outer class being the target \n",
    "        - *continuous variables* - by plotting histograms with distributions colored by class\n",
    "      -  for **regression**\n",
    "        - *categorical variables* - box plots by category for different categorical variables\n",
    "        - *continous variables* - good ol' scatter plots\n",
    "- get quick summary stats for continuous variables and value counts for categorical variables\n",
    "- treatment of invalid/missing data (including in the target)\n",
    "  - drop columns like ids, dates(if underlying info like day/year/month also irrelevant), and const values\n",
    "  - remove extreme values in case of regression targets (use a boxplot)\n",
    "  - use imputers to fill in missing values (eg. Simple/KKN-Imputer - may need to encode categorical features first)\n",
    "- treatment of categorical variables. For example, OrdinalEncoder, OneHotEncoder, categorical_encoders.TargetEncoder (needs to go separately and first in the pipeline)\n",
    "- treatment of continous variables:\n",
    "  - definitely scale for KNN, Kernel SVM: `StandardScaler` or `MinMaxScaler` ftw\n",
    "\n",
    "### Tree Ensembles\n",
    "**Random Forests**\n",
    "  - Main parameter: max_feature\n",
    "  - around sqrt(n_features) for classification\n",
    "  - Around n_features for regression\n",
    "  - n_estimators > 100\n",
    "\n",
    "Thumb rule for when to use tree-based models:\n",
    "- model non-linear relationships\n",
    "- donâ€™t care about scaling, no need for feature engineering\n",
    "- random forests are very robust, good benchmark\n",
    "- **gradient boosting** will often give the best performance with careful tuning (*early stopping, learning rate, regularization, max_features*, pruning via *max_depth*)\n",
    "- LightGBM Interface:\n",
    "```\n",
    "    lgbm = LGBMClassifier()\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    lgbm.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### more miscellaneous TIPs\n",
    "- LinearSVC, LogisticRegression: `dual=False` if `n_samples` >> `n_features`\n",
    "- `LogisticRegression(solver=\"sag\")` for `n_samples` large.\n",
    "- Stochastic Gradient Descent for `n_samples` really large\n",
    "- tip on solvers: https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions\n",
    "- LogisticRegression(solver='lbfgs', multi_class='multinomial').fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "P0\n",
    "- complete feature engineering + tuning cycle with random forest, lightGBM\n",
    "- finish the general outline to include modeling, cross-validation, and evaluation\n",
    "- choosing an evaluation metric for regression - RMSE vs r^2\n",
    "\n",
    "P1\n",
    "- write helper functions for: visualization (see cases above), maybe using seaborn if seems quicker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "(Will help remind where to look for in the sklearn docs for the right API)\n",
    "\n",
    "**general**\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "**missing values imputation**\n",
    " ```\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute impute KNNImputer\n",
    " ```\n",
    "\n",
    "**feature engineering**\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "```\n",
    "\n",
    "**cross-validation and model tuning**\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipelines import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "```\n",
    "\n",
    "**classifiction models**\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "```\n",
    "\n",
    "**target transformation** \\\n",
    "`from sklearn.compose import TransformedTargetRegressor`\n",
    "\n",
    "**Sklearn API reference** - https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage\n",
    "\n",
    "loading data \\\n",
    "`df = pd.read_csv()`\n",
    "\n",
    "examining data\n",
    "```\n",
    "df.info()\n",
    "df.head()\n",
    "y.unique()\n",
    "y.nunique()\n",
    "y.value_counts()\n",
    "```\n",
    "\n",
    "train_test_split\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# make y boolean when it's not (eg, -1/1)\n",
    "# this allows sklearn to determine the positive class more easily\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y == '1', random_state=0)\n",
    "```\n",
    "\n",
    "boxplot \\\n",
    "`plt.boxplot(y, vert=False)`\n",
    "\n",
    "apply function \\\n",
    "`df[\"column_name\"].apply(func)`\n",
    "\n",
    "OneHotEncoder \n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories=[['first','second','third','forth']], drop=\"if_binary\")\n",
    "X = [['third'], ['second'], ['first']]\n",
    "enc.fit(X)\n",
    "print(enc.transform([['second'], ['first'], ['third'],['forth']]))\n",
    "```\n",
    "\n",
    "OrdinalEncoder\n",
    "```\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder(categories=[['first','second','third','forth']])\n",
    "X = [['third'], ['second'], ['first']]\n",
    "enc.fit(X)\n",
    "print(enc.transform([['second'], ['first'], ['third'],['forth']]))\n",
    "```\n",
    "\n",
    "Target transformer\n",
    "```\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    regressor=LinearRegression(),\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp.fit())\n",
    "```\n",
    "returns an estimator\n",
    "\n",
    "ColumnTransformer syntax <-> Pipeline syntax\n",
    "\n",
    "```\n",
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NA'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler())\n",
    "```\n",
    "*note the example of make_column_selector*\n",
    "```\n",
    "preprocess = make_column_transformer(\n",
    "    (cat_preprocessing, make_column_selector(dtype_include='object')),\n",
    "    remainder=cont_preprocessing)\n",
    "```\n",
    "\n",
    "Pipeline syntax <-> ColumnTransformer syntax \\\n",
    "`make_pipeline(preprocess, log_regressor)`\n",
    "\n",
    "GridSearchCV syntax \\\n",
    "`GridSearchCV(make_pipeline_output_object, param_grid=param_grid, cv=KFold(n_splits=5,shuffle=True))`\n",
    "\n",
    "Common estimator API:\n",
    "```\n",
    ".fit(X, y)\n",
    ".predict(X')\n",
    ".score(X', y')\n",
    ".predict_proba(X')\n",
    "```\n",
    "\n",
    "### Evaluation metrics\n",
    "\n",
    "##### Binary classification\n",
    "*Threshold-based:*\n",
    "- accuracy\n",
    "- precision, recall, f1\n",
    "\n",
    "*Ranking:*\n",
    "- average precision\n",
    "- ROC AUC\n",
    "\n",
    "##### Multiclass classification\n",
    "*Threshold-based:*\n",
    "- accuracy\n",
    "- precision, recall, f1 (macro average, weighted)\n",
    "\n",
    "*Ranking:*\n",
    "- OVR ROC AUC\n",
    "- OVO ROC AUC\n",
    "\n",
    "\n",
    "**sklearn**\n",
    "\n",
    "*metrics in cross-validation*\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "explicit_accuracy =  cross_val_score(rf, X, y,  scoring=\"accuracy\")\n",
    "```\n",
    "*common metrics*\n",
    "```\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, classification_report\n",
    "\n",
    "recall_score(y_test, y_pred)\n",
    "precision_score(y_test, y_pred)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_score = roc_auc_score(y_test, estimator.predict_proba(X_test)[:, 1])\n",
    "classification_report(y_true, y_pred)\n",
    "```\n",
    "\n",
    "Plotting the PR curve:\n",
    "```\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    classifier, X_test, y_test, name=\"LinearSVC\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n",
    "```\n",
    "Plotting the ROC curve:\n",
    "```\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "y_score = clf.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final set of transformed features and coefficients \n",
    "def feature_coefficients(grid_search_cv_object):\n",
    "    coeff= grid_search_cv_object.best_estimator_.named_steps['linearsvc'].coef_[0] # key will be the id of the final estimator\n",
    "    feature_names =grid_search_cv_object.best_estimator_.named_steps.columntransformer.named_transformers_[\"onehotencoder\"].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to separate continuous and categorical features\n",
    "def separate_cont_cat(df):\n",
    "    continuous_features = df.select_dtypes(include='number').columns\n",
    "    categorical_features = df.select_dtypes(exclude='number').columns\n",
    "    return continuous_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return X, y (training features, labels)\n",
    "def split_df_x_y(df, target_col):\n",
    "    cols = df.columns\n",
    "    feature_cols = [col for col in cols if col != target_col]\n",
    "    return df[feature_cols], df[target_col]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5 (default, Oct 25 2019, 10:52:18) \n[Clang 4.0.1 (tags/RELEASE_401/final)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5269c0a7ad73348e1a39fbbfe9a8878a0938622bfaba1cb70a06abfc1d5d698f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
