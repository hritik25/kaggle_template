{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General outline:\n",
    "\n",
    "- load the data\n",
    "- broadly segregate continuous vs categorical variables\n",
    "- visualize the data to get a feel\n",
    "  1. visualize the target (histogram/bar for regression/classification) - note if you may need to apply any transformations\n",
    "  2. visualize the univariate distributions of the features (histograms, bars)\n",
    "  3. visulize target vs feature values distributions\n",
    "      - for **cassification**\n",
    "        - *categorical variables* - by doing nested bar plots with outer class being the target \n",
    "        - *continuous variables* - by plotting histograms with distributions colored by class\n",
    "      -  for **regression**\n",
    "        - *categorical variables* - box plots by category for different categorical variables\n",
    "        - *continous variables* - good ol' scatter plots\n",
    "- get quick summary stats for continuous variables and value counts for categorical variables\n",
    "- treatment of invalid/missing data (including in the target)\n",
    "  - drop columns like ids, dates(if underlying info like day/year/month also irrelevant), and const values\n",
    "  - remove extreme values in case of regression targets (use a boxplot)\n",
    "  - use imputers to fill in missing values (eg. Simple/KKN-Imputer - may need to encode categorical features first)\n",
    "- decide on a metric to evalate the model on (eg. AUC-ROC, adjusted R^2)\n",
    "- treatment of categorical variables. For example, OrdinalEncoder, OneHotEncoder, categorical_encoders.TargetEncoder (needs to go separately and first in the pipeline)\n",
    "- treatment of continous variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "  0. check this notebook into a github repo\n",
    "  1. write helper functions for: visualization (see cases above), maybe using seaborn if seems quicker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports related to cross-validation\n",
    "(Will help remind where to look for in the sklearn docs for the right API)\n",
    "\n",
    "**general**\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "**missing values imputation**\n",
    " ```\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute impute KNNImputer\n",
    " ```\n",
    "\n",
    "**feature engineering**\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "```\n",
    "\n",
    "**cross-validation and model tuning**\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipelines import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "```\n",
    "\n",
    "**classifiction models**\n",
    "```\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "```\n",
    "\n",
    "**target transformation** \\\n",
    "`from sklearn.compose import TransformedTargetRegressor`\n",
    "\n",
    "**Sklearn API reference** - https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data \\\n",
    "`df = pd.read_csv()`\n",
    "\n",
    "examining data\n",
    "```\n",
    "df.info()\n",
    "df.head()\n",
    "```\n",
    "\n",
    "train_test_split \\\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)`\n",
    "\n",
    "boxplot \\\n",
    "`plt.boxplot(y, vert=False)`\n",
    "\n",
    "apply function \\\n",
    "`df[\"column_name\"].apply(func)`\n",
    "\n",
    "**Example usage**\n",
    "\n",
    "OneHotEncoder \n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories=[['first','second','third','forth']], drop=\"if_binary\")\n",
    "X = [['third'], ['second'], ['first']]\n",
    "enc.fit(X)\n",
    "print(enc.transform([['second'], ['first'], ['third'],['forth']]))\n",
    "```\n",
    "\n",
    "OrdinalEncoder\n",
    "```\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder(categories=[['first','second','third','forth']])\n",
    "X = [['third'], ['second'], ['first']]\n",
    "enc.fit(X)\n",
    "print(enc.transform([['second'], ['first'], ['third'],['forth']]))\n",
    "```\n",
    "\n",
    "Target transformer\n",
    "```\n",
    "log_regressor = TransformedTargetRegressor(\n",
    "    regressor=LinearRegression(),\n",
    "    func=np.log,\n",
    "    inverse_func=np.exp.fit())\n",
    "```\n",
    "returns an estimator\n",
    "\n",
    "ColumnTransformer syntax <-> Pipeline syntax\n",
    "```\n",
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NA'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler())\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (cat_preprocessing, make_column_selector(dtype_include='object')),\n",
    "    remainder=cont_preprocessing)\n",
    "```\n",
    "\n",
    "Pipeline syntax <-> ColumnTransformer syntax \\\n",
    "`make_pipeline(preprocess, log_regressor)`\n",
    "\n",
    "GridSearchCV syntax \\\n",
    "`GridSearchCV(make_pipeline_output_object, param_grid=param_grid, cv=KFold(n_splits=5,shuffle=True))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to separate continuous and categorical features\n",
    "def separate_cont_cat(df):\n",
    "    continuous_features = df.select_dtypes(include='number').columns\n",
    "    categorical_features = df.select_dtypes(exclude='number').columns\n",
    "    return continuous_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return X, y (training features, labels)\n",
    "def split_df_x_y(df, target_col):\n",
    "    cols = df.columns\n",
    "    feature_cols = [col for col in cols if col != target_col]\n",
    "    return df[feature_cols], df[target_col]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5269c0a7ad73348e1a39fbbfe9a8878a0938622bfaba1cb70a06abfc1d5d698f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
