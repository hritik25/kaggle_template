{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.41.0-cp37-cp37m-macosx_10_9_x86_64.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.3/436.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (1.0.2)\n",
      "Requirement already satisfied: pandas in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (1.0.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (1.18.1)\n",
      "Requirement already satisfied: numba in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (0.48.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (1.3.0)\n",
      "Collecting packaging>20.9\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m770.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4.25.0 in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (4.42.1)\n",
      "Requirement already satisfied: scipy in /Users/candide/anaconda3/lib/python3.7/site-packages (from shap) (1.4.1)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /Users/candide/anaconda3/lib/python3.7/site-packages (from numba->shap) (0.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/candide/anaconda3/lib/python3.7/site-packages (from numba->shap) (65.7.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/candide/anaconda3/lib/python3.7/site-packages (from pandas->shap) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/candide/anaconda3/lib/python3.7/site-packages (from pandas->shap) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/candide/anaconda3/lib/python3.7/site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/candide/anaconda3/lib/python3.7/site-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/candide/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->shap) (1.14.0)\n",
      "Installing collected packages: slicer, packaging, shap\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed packaging-23.0 shap-0.41.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '/Users/candide/Acads/Programming Practise/reddit_screen')\n",
    "\n",
    "from boilerplate import *\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filepath to the given file\n",
    "FILEPATH = \"/Users/candide/Acads/Programming Practise/reddit_screen/aml_hws/vehicles.csv\"\n",
    "\n",
    "# load the data - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html \n",
    "data = pd.read_csv(FILEPATH, sep=\",\", header='infer', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the data looks like\n",
    "data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an insight into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample dataset in case it's massive\n",
    "data = data.sample(100000, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the target column\n",
    "TARGET_COL = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the distribution of the target for potential outlier values\n",
    "\n",
    "# if it's continuous\n",
    "plt.boxplot(data[TARGET_COL], vert=False)\n",
    "\n",
    "# if it's discrete\n",
    "#data[TARGET_COL].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.price < 1000000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into categorical and continuous features broadly\n",
    "X, y = split_df_x_y(data, \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a quick view into the data types of features\n",
    "summarize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats on continuous features\n",
    "X.describe() # by default only describes continous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns for continuous features which seem empty/irrelevant from the stats\n",
    "X_cont_dropped = X.drop([\"county\", \"id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate continuous vs categorical features broadly\n",
    "cont_features, cat_features = separate_cont_cat(X_cont_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize continuous features\n",
    "visualize_feature_distributions_rough(X_cont_dropped, cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats on categorical features\n",
    "cat_stats = categorical_variables_stats(X_cont_dropped, cat_features)\n",
    "cat_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns for categorical features which seem empty/irrelevant from the stats\n",
    "X_cont_cat_dropped = X_cont_dropped.drop([\"url\", \"posting_date\", \"description\", \"image_url\", \"region\", \"VIN\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitler continuous vs categorical features again\n",
    "cont_features, cat_features = separate_cont_cat(X_cont_cat_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cont_cat_dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cont_cat_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cont_cat_dropped, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NA'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocessing.fit(X_train[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_preprocessing.fit(X_train[cont_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns) == len(cat_features) + len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_transformer = make_column_transformer(\n",
    "    (cat_preprocessing, cat_features),\n",
    "    remainder=cont_preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(col_transformer.fit_transform(X_train).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = make_pipeline(col_transformer, RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe.fit(X_train, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(lr_pipe, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5269c0a7ad73348e1a39fbbfe9a8878a0938622bfaba1cb70a06abfc1d5d698f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
